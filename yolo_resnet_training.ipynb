{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "hEiNWg1T6_Ec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "from scipy.special import expit as sigmoid\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "def loop_body(t_true, t_pred, i, ta):\n",
    "    '''\n",
    "    This function is the main body of the custom_loss() definition, called from within the torch.while_loop()\n",
    "    The loss function implemented here is as described in the original YOLO paper: https://arxiv.org/abs/1506.02640\n",
    "\n",
    "    # Arguments\n",
    "    t_true: the ground truth tensor; shape: (batch_size, 1573)\n",
    "    t_pred: the predicted tensor; shape: (batch_size, 1573)\n",
    "    i: iteration count of the while_loop\n",
    "    ta: List to store loss tensors\n",
    "    '''\n",
    "\n",
    "    ### Get the current iteration's true and predicted tensor\n",
    "    c_true = t_true[i]\n",
    "    c_pred = t_pred[i]\n",
    "\n",
    "    ### Apply sigmoid to the coordinates part of the tensor to scale it between 0 and 1 as expected\n",
    "    c_pred = torch.cat((c_pred[:605], torch.sigmoid(c_pred[-968:])), dim=0)\n",
    "\n",
    "\n",
    "    ### Reshape to GRIDxGRIDxBBOXES blocks for simpler correspondence of values across grid cell and bounding boxes\n",
    "    xywh_true = c_true[-968:].view(11, 11, 2, 4)\n",
    "    xywh_pred = c_pred[-968:].view(11, 11, 2, 4)\n",
    "\n",
    "    ### Convert normalized values to actual ones (still relative to grid cell size)\n",
    "    X_NORM = 1.0  # Replace with the actual normalization value for 'X'\n",
    "    Y_NORM = 1.0  # Replace with the actual normalization value for 'Y'\n",
    "    WIDTH_NORM = 1.0  # Replace with the actual normalization value for 'Width'\n",
    "    HEIGHT_NORM = 1.0  # Replace with the actual normalization value for 'Height'\n",
    "\n",
    "    GRID_NUM = 11\n",
    "    X_SPAN = WIDTH_NORM/GRID_NUM\n",
    "    Y_SPAN = HEIGHT_NORM/GRID_NUM\n",
    "    X_NORM = WIDTH_NORM/GRID_NUM\n",
    "    Y_NORM = HEIGHT_NORM/GRID_NUM\n",
    "    WIDTH_NORM = 224\n",
    "    HEIGHT_NORM = 224\n",
    "\n",
    "\n",
    "\n",
    "    x_true = xywh_true[:, :, :, 0] * X_NORM\n",
    "    x_pred = xywh_pred[:, :, :, 0] * X_NORM\n",
    "\n",
    "    y_true = xywh_true[:, :, :, 1] * Y_NORM\n",
    "    y_pred = xywh_pred[:, :, :, 1] * Y_NORM\n",
    "\n",
    "    w_true = xywh_true[:, :, :, 2] * WIDTH_NORM\n",
    "    w_pred = xywh_pred[:, :, :, 2] * WIDTH_NORM\n",
    "\n",
    "    h_true = xywh_true[:, :, :, 3] * HEIGHT_NORM\n",
    "    h_pred = xywh_pred[:, :, :, 3] * HEIGHT_NORM\n",
    "\n",
    "    # Remaining code to be converted similarly...\n",
    "\n",
    "    # Example: For tensor arithmetic operations, torch.abs(), torch.sqrt(), torch.square(), etc.\n",
    "    # Also, use torch.where() instead of tf.nn.relu() for the ReLU operation.\n",
    "\n",
    "    # Remaining code that follows tensor operations should be similarly converted.\n",
    "\n",
    "    # For PyTorch, make sure to use appropriate tensor operations to perform the same calculations.\n",
    "    # Ensure to replace tensor slicing and concatenation with PyTorch equivalents.\n",
    "\n",
    "    # ...\n",
    "\n",
    "    ### The below is a different approach on calculating IOU between\n",
    "    ### predicted bounding boxes and ground truth\n",
    "    ### See README.md for explanation for the formula\n",
    "\n",
    "    x_dist = torch.abs(torch.subtract(x_true, x_pred))\n",
    "    y_dist = torch.abs(torch.subtract(y_true, y_pred))\n",
    "\n",
    "\n",
    "    ### (w1/2 +w2/2 -d) > 0 => intersection, else no intersection\n",
    "    ### (h1/2 +h2/2 -d) > 0 => intersection, else no intersection\n",
    "\n",
    "    wwd = torch.nn.functional.relu(w_true/2 + w_pred/2 - x_dist)\n",
    "    hhd = torch.nn.functional.relu(h_true/2 + h_pred/2 - y_dist)\n",
    "\n",
    "    area_true = torch.multiply(w_true, h_true)\n",
    "    area_pred = torch.multiply(w_pred, h_pred)\n",
    "    area_intersection = torch.multiply(wwd, hhd)\n",
    "\n",
    "    iou = area_intersection / (area_true + area_pred - area_intersection + 1e-4)\n",
    "    confidence_true = torch.reshape(iou, (-1,))\n",
    "\n",
    "    ### Masks for grids that do contain an object, from ground truth\n",
    "    ### The class probability block from the ground truth is used as an indicator for all grid cells that\n",
    "    ### actually have an object present in itself.\n",
    "    grid_true = torch.reshape(c_true[:363], (11,11,3))\n",
    "    grid_true_sum = torch.sum(grid_true, dim=2)\n",
    "    grid_true_exp = torch.stack((grid_true_sum, grid_true_sum), dim=2)\n",
    "    grid_true_exp3 = torch.stack((grid_true_sum, grid_true_sum, grid_true_sum), dim=2)\n",
    "    grid_true_exp4 = torch.stack((grid_true_sum, grid_true_sum, grid_true_sum, grid_true_sum), dim=2)\n",
    "\n",
    "    coord_mask = torch.reshape(grid_true_exp4, (-1,))\n",
    "    confidence_mask = torch.reshape(grid_true_exp, (-1,))\n",
    "    confidence_true = confidence_true * confidence_mask\n",
    "\n",
    "    ### Revised ground truth tensor, based on calculated confidence values and with non-object grids suppressed\n",
    "    c_true_new = torch.cat([c_true[:363], confidence_true, c_true[-968:]], dim=0)\n",
    "\n",
    "    ### Create masks for 'responsible' bounding box in a grid cell for loss calculation\n",
    "    confidence_true_matrix = torch.reshape(confidence_true, (11,11,2))\n",
    "    confidence_true_argmax = torch.argmax(confidence_true_matrix, dim=2)\n",
    "    confidence_true_argmax = confidence_true_argmax.type(torch.int32)\n",
    "    ind_i, ind_j = torch.meshgrid(torch.arange(11, dtype=torch.int32), torch.arange(11, dtype=torch.int32), indexing='ij')\n",
    "    ind_argmax = torch.stack((ind_i, ind_j, confidence_true_argmax), axis=2)\n",
    "    ind_argmax = torch.reshape(ind_argmax, (121,3))\n",
    "\n",
    "    ind_argmax = ind_argmax.type(torch.int64)\n",
    "\n",
    "\n",
    "    # responsible_mask_2 = torch.full((11,11,2), 0).scatter_(1, ind_argmax,1)\n",
    "    # responsible_mask_2 = torch.reshape(responsible_mask_2, (-1,))\n",
    "    # responsible_mask_2 = responsible_mask_2 * confidence_mask\n",
    "\n",
    "    # responsible_mask_4 = torch.full((11,11,2,2), 0).scatter_(1, ind_argmax,1)\n",
    "    # responsible_mask_4 = torch.reshape(responsible_mask_4, (-1,))\n",
    "    # responsible_mask_4 = responsible_mask_4 * coord_mask\n",
    "\n",
    "    responsible_mask_2 = torch.zeros(11, 11, 2)\n",
    "    responsible_mask_2[ind_argmax[:, 0], ind_argmax[:, 1], ind_argmax[:, 2]] = 1\n",
    "    responsible_mask_2 = responsible_mask_2.view(-1)\n",
    "    responsible_mask_2 = responsible_mask_2 * confidence_mask\n",
    "\n",
    "    responsible_mask_4 = torch.zeros(11, 11, 2, 2)\n",
    "    responsible_mask_4[ind_argmax[:, 0], ind_argmax[:, 1], ind_argmax[:, 2], :] = 1\n",
    "    responsible_mask_4 = responsible_mask_4.view(-1)\n",
    "    responsible_mask_4 = responsible_mask_4 * coord_mask\n",
    "\n",
    "\n",
    "    ### Masks for rest of the bounding boxes\n",
    "    responsible_mask_2.type(torch.bool)\n",
    "    responsible_mask_4.type(torch.bool)\n",
    "\n",
    "    negated_responsible_mask_2 = torch.logical_not(responsible_mask_2)\n",
    "    negated_responsible_mask_4 = torch.logical_not(responsible_mask_4)\n",
    "    inv_responsible_mask_2 = negated_responsible_mask_2.type(torch.float32)\n",
    "    inv_responsible_mask_4 = negated_responsible_mask_4.type(torch.float32)\n",
    "\n",
    "    ### lambda values\n",
    "    lambda_coord = 5.0\n",
    "    lambda_noobj = 0.5\n",
    "\n",
    "\n",
    "    ### loss from dimensions ###\n",
    "    dims_true = torch.reshape(c_true_new[-968:], (11,11,2,4))\n",
    "    dims_pred = torch.reshape(c_pred[-968:], (11,11,2,4))\n",
    "\n",
    "    xy_true = torch.reshape(dims_true[:,:,:,:2], (-1,))\n",
    "    xy_pred = torch.reshape(dims_pred[:,:,:,:2], (-1,))\n",
    "\n",
    "    wh_true = torch.reshape(dims_true[:,:,:,2:], (-1,))\n",
    "    wh_pred = torch.reshape(dims_pred[:,:,:,2:], (-1,))\n",
    "\n",
    "\n",
    "    #### XY difference loss\n",
    "    xy_loss = (xy_true - xy_pred) * responsible_mask_4\n",
    "    xy_loss = torch.square(xy_loss)\n",
    "    xy_loss = lambda_coord * torch.sum(xy_loss)\n",
    "\n",
    "\n",
    "    #### WH sqrt diff loss\n",
    "    wh_loss = (torch.sqrt(torch.abs(wh_true)) - torch.sqrt(torch.abs(wh_pred))) * responsible_mask_4\n",
    "    wh_loss = torch.square(wh_loss)\n",
    "    wh_loss = lambda_coord * torch.sum(wh_loss)\n",
    "\n",
    "\n",
    "    ### Conf losses\n",
    "    conf_true = c_true_new[363:605]\n",
    "    conf_pred = c_pred[363:605]\n",
    "\n",
    "\n",
    "    conf_loss_obj = (conf_true - conf_pred) * responsible_mask_2\n",
    "    conf_loss_obj = torch.square(conf_loss_obj)\n",
    "    conf_loss_obj = torch.sum(conf_loss_obj)\n",
    "\n",
    "\n",
    "    conf_loss_noobj = (conf_true - conf_pred) * inv_responsible_mask_2\n",
    "    conf_loss_noobj = torch.square(conf_loss_noobj)\n",
    "    conf_loss_noobj = lambda_noobj * torch.sum(conf_loss_noobj)\n",
    "\n",
    "\n",
    "    #### Class Prediction Loss\n",
    "    class_true = torch.reshape(c_true_new[:363], (11,11,3))\n",
    "    class_pred = torch.reshape(c_pred[:363], (11,11,3))\n",
    "    class_pred_softmax = class_pred #tf.nn.softmax(class_pred)\n",
    "\n",
    "    classification_loss = class_true - class_pred_softmax\n",
    "    classification_loss = classification_loss * grid_true_exp3\n",
    "    classification_loss = torch.square(classification_loss)\n",
    "    classification_loss = torch.sum(classification_loss)\n",
    "\n",
    "    ## Total loss = xy-loss + wh-loss + Confidence_loss_obj + Confidence_loss_noobj + classification_loss\n",
    "    total_loss = xy_loss + wh_loss + conf_loss_obj + conf_loss_noobj + classification_loss\n",
    "\n",
    "\n",
    "\n",
    "    total_loss.requires_grad_(True)\n",
    "\n",
    "    # Finally, append the calculated loss to the list ta\n",
    "    ta = total_loss.clone().detach().requires_grad_(True)  # Replace this with the calculated loss\n",
    "\n",
    "\n",
    "\n",
    "    i = i + 1\n",
    "    return t_true, t_pred, i, ta\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Custom loss function as per the YOLO paper, since there are no default loss functions in PyTorch that fit.\n",
    "    '''\n",
    "    i = 0\n",
    "    ta = []\n",
    "    y_true = y_true.reshape(-1,1573)\n",
    "    while i < y_true.size(0):  # Assuming y_true is a torch tensor\n",
    "        y_true, y_pred, i, ta_i_value = loop_body(y_true, y_pred, i, ta)\n",
    "        ta.append(ta_i_value)\n",
    "\n",
    "    # Once the loop is done, calculate mean loss from the list of losses\n",
    "    loss_tensor = torch.tensor(ta)\n",
    "    loss_mean = torch.mean(loss_tensor)\n",
    "\n",
    "    if not isinstance(loss_mean, torch.Tensor):\n",
    "        # print(\"not a tensor\")\n",
    "        loss_mean = torch.tensor(loss_mean, requires_grad=True)\n",
    "    elif not loss_mean.requires_grad:\n",
    "        # print(\"is a tensor\")\n",
    "        loss_mean.requires_grad_(True)\n",
    "\n",
    "\n",
    "    return loss_mean  # Return the mean loss\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "\n",
    "### Helper funtions for data augumentation for training the network ###\n",
    "def coord_translate(bboxes, tr_x, tr_y):\n",
    "    '''\n",
    "    Takes a single frame's bounding box list with confidence scores and\n",
    "    applies translation (addition) to the coordinates specified by 'tr'\n",
    "\n",
    "    parameters:\n",
    "    bboxes: list with element of the form ((x1,y1), (x2,y2)), (c1,c2,c3)\n",
    "    tr_x, tr_y: translation factor to add the coordinates to, for x and y respectively\n",
    "\n",
    "    returns: new list with translated coordinates and same conf scores; same shape as bboxes\n",
    "    '''\n",
    "    new_list = []\n",
    "    for box in bboxes:\n",
    "        coords = np.array(box[0])\n",
    "        coords[:,0] = coords[:,0] + tr_x\n",
    "        coords[:,1] = coords[:,1] + tr_y\n",
    "        coords = coords.astype(np.int64)\n",
    "        out_of_bound_indices = np.average(coords, axis=0) >= 224\n",
    "        if out_of_bound_indices.any():\n",
    "            continue\n",
    "        coords = coords.tolist()\n",
    "        new_list.append((coords, box[1]))\n",
    "    return new_list\n",
    "def coord_scale(bboxes, sc):\n",
    "    '''\n",
    "    Takes a singl frame's bounding box list with confidence scores and\n",
    "    applies scaling to the coordinates specified by sc\n",
    "\n",
    "    parameters:\n",
    "    bboxes: list with element of the form ((x1,y1), (x2,y2)), (c1,c2,c3)\n",
    "    sc: scaling factor to multiply the coordinates with\n",
    "\n",
    "    returns: new list with scaled coordinates and same conf scores; same shape as bboxes\n",
    "    '''\n",
    "    new_list = []\n",
    "    for box in bboxes:\n",
    "        coords = np.array(box[0])\n",
    "        coords = coords * sc\n",
    "        coords = coords.astype(np.int64)\n",
    "        out_of_bound_indices = np.average(coords, axis=0) >= 224\n",
    "        if out_of_bound_indices.any():\n",
    "            continue\n",
    "        coords = coords.tolist()\n",
    "        new_list.append((coords, box[1]))\n",
    "    return new_list\n",
    "def label_to_tensor(frame, imgsize=(224, 224), gridsize=(11,11), classes=3, bboxes=2):\n",
    "\n",
    "    '''\n",
    "    This function takes in the frame (rows corresponding to a single image in the labels.csv)\n",
    "    and converts it into the format our network expects (coord conversion and normalization)\n",
    "\n",
    "    '''\n",
    "    grid = np.zeros(gridsize)\n",
    "\n",
    "    y_span = imgsize[0]/gridsize[0]\n",
    "    x_span = imgsize[1]/gridsize[1]\n",
    "\n",
    "    class_prob = np.zeros((gridsize[0], gridsize[1], classes))\n",
    "    confidence = np.zeros((gridsize[0], gridsize[1], bboxes))\n",
    "    dims = np.zeros((gridsize[0], gridsize[1], bboxes, 4))\n",
    "\n",
    "    for box in frame:\n",
    "        ((x1,y1), (x2,y2)), (c1,c2,c3) = box\n",
    "        x_grid = int(((x1+x2)/2)//x_span)\n",
    "        y_grid = int(((y1+y2)/2)//y_span)\n",
    "\n",
    "        class_prob[y_grid, x_grid] = (c1,c2,c3)\n",
    "\n",
    "        x_center = ((x1+x2)/2)\n",
    "        y_center = ((y1+y2)/2)\n",
    "\n",
    "        x_center_norm = (x_center-x_grid*x_span)/(x_span)\n",
    "        y_center_norm = (y_center-y_grid*y_span)/(y_span)\n",
    "\n",
    "        w = x2-x1\n",
    "        h = y2-y1\n",
    "\n",
    "        w_norm = w/imgsize[1]\n",
    "        h_norm = h/imgsize[0]\n",
    "\n",
    "        dims[y_grid, x_grid, :, :] = (x_center_norm, y_center_norm, w_norm, h_norm)\n",
    "\n",
    "        grid[y_grid, x_grid] += 1\n",
    "\n",
    "    tensor = np.concatenate((class_prob.ravel(), confidence.ravel(), dims.ravel()))\n",
    "    return tensor\n",
    "\n",
    "# TODO change the \"folder\" argument to where the images are residing\n",
    "def augument_data(label, frame, imgsize=(224, 224), folder=r'D:/PD Data/Resnet+yolo Code/Plant Disease CSV Format 3 Class more Train Data/train/'):\n",
    "    '''\n",
    "    Takes the image file name and the frame (rows corresponding to a single image in the labels.csv)\n",
    "    and randomly scales, translates, adjusts SV values in HSV space for the image,\n",
    "    and adjusts the coordinates in the 'frame' accordingly, to match bounding boxes in the new image\n",
    "    '''\n",
    "    img = cv2.imread(folder+label)\n",
    "    img = cv2.resize(img, imgsize)\n",
    "    rows, cols = img.shape[:2]\n",
    "\n",
    "    #translate_factor\n",
    "    tr = np.random.random() * 0.2 + 0.01\n",
    "    tr_y = np.random.randint(rows*-tr, rows*tr)\n",
    "    tr_x = np.random.randint(cols*-tr, cols*tr)\n",
    "    #scale_factor\n",
    "    sc = np.random.random() * 0.4 + 0.8\n",
    "\n",
    "    # flip coin to adjust image saturation\n",
    "    r = np.random.rand()\n",
    "    if r < 0.5:\n",
    "        #randomly adjust the S and V values in HSV representation\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "        fs = np.random.random() + 0.7\n",
    "        fv = np.random.random() + 0.2\n",
    "        img[:,:,1] *= fs\n",
    "        img[:,:,2] *= fv\n",
    "        img = img.astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    # new random factor for scaling and translating\n",
    "    r = np.random.rand()\n",
    "\n",
    "    if r < 0.3:\n",
    "        #translate image\n",
    "        M = np.float32([[1,0,tr_x], [0,1,tr_y]])\n",
    "        img = cv2.warpAffine(img, M, (cols,rows))\n",
    "        frame = coord_translate(frame, tr_x, tr_y)\n",
    "    elif r < 0.6:\n",
    "        #scale image keeping the same size\n",
    "        placeholder = np.zeros_like(img)\n",
    "        meta = cv2.resize(img, (0,0), fx=sc, fy=sc)\n",
    "        if sc < 1:\n",
    "            placeholder[:meta.shape[0], :meta.shape[1]] = meta\n",
    "        else:\n",
    "            placeholder = meta[:placeholder.shape[0], :placeholder.shape[1]]\n",
    "        img = placeholder\n",
    "        frame = coord_scale(frame, sc)\n",
    "\n",
    "    return img, frame\n",
    "#-----------------------------------------------------------------------#\n",
    "\n",
    "### Define generator and Import dataset (do test/train split)\n",
    "# would need to look into this\n",
    "def generator(label_keys, label_frames, batch_size=1, folder='udacity-object-detection-crowdai/'):\n",
    "    '''\n",
    "    Generator function\n",
    "    # Arguments\n",
    "    label_keys: image names, that are keys of the label_frames Arguments\n",
    "    label_frames: array of frames (rows corresponding to a single image in the labels.csv)\n",
    "    batch_size: batch size\n",
    "    '''\n",
    "    num_samples = len(label_keys)\n",
    "    indx = label_keys\n",
    "\n",
    "    count = 0\n",
    "    while count < 1:\n",
    "        shuffle(indx)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = indx[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            gt = []\n",
    "            for batch_sample in batch_samples:\n",
    "                im, frame = augument_data(batch_sample, label_frames[batch_sample])\n",
    "                im = im.astype(np.float32)\n",
    "                im -= 128\n",
    "                images.append(im)\n",
    "                frame_tensor = label_to_tensor(frame)\n",
    "                gt.append(frame_tensor)\n",
    "\n",
    "            X_train = torch.Tensor(images)\n",
    "            y_train = torch.Tensor(gt)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "def plot_history(history_object):\n",
    "    print(history_object.history.keys())\n",
    "    ### plot the training and validation loss for each epoch\n",
    "    plt.plot(history_object.history['loss'])\n",
    "    plt.plot(history_object.history['val_loss'])\n",
    "    plt.title('model mean squared error loss')\n",
    "    plt.ylabel('mean squared error loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_value(box1, box2):\n",
    "    '''\n",
    "    calculate the IOU of two given boxes\n",
    "    '''\n",
    "    (x11, y11) , (x12, y12) = box1\n",
    "    (x21, y21) , (x22, y22) = box2\n",
    "\n",
    "    x1 = max(x11, x21)\n",
    "    x2 = min(x12, x22)\n",
    "    w = max(0, (x2-x1))\n",
    "\n",
    "    y1 = max(y11, y21)\n",
    "    y2 = min(y12, y22)\n",
    "    h = max(0, (y2-y1))\n",
    "\n",
    "    area_intersection = w*h\n",
    "    area_combined = abs((x12-x11)*(y12-y11) + (x22-x21)*(y22-y21) + 1e-3)\n",
    "\n",
    "    return area_intersection/area_combined\n",
    "\n",
    "\n",
    "def find_average_iou(bounding_boxes_grouped_actual, bounding_boxes_grouped_predicted):\n",
    "\n",
    "    all_ious = []\n",
    "    for bb_actual,bb_predicted in zip(bounding_boxes_grouped_actual, bounding_boxes_grouped_predicted):\n",
    "\n",
    "        if bb_actual[0].sum() <= 0:\n",
    "            continue\n",
    "\n",
    "        x_act,y_act,w_act,h_act = bb_actual[0]\n",
    "        x_pred,y_pred,w_pred,h_pred = bb_predicted[0]\n",
    "\n",
    "        bbx_act = ((x_act-w_act/2,y_act-h_act/2), (x_act+w_act/2,y_act+h_act/2))\n",
    "        bbx_pred = ((x_pred-w_pred/2,y_pred-h_pred/2), (x_pred+w_pred/2,y_pred+h_pred/2))\n",
    "\n",
    "        iou = iou_value(bbx_act, bbx_pred)\n",
    "\n",
    "        if iou >0 :\n",
    "            all_ious.append(iou)\n",
    "\n",
    "    return all_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/AdeelH/pytorch-fpn/zipball/master\" to C:\\Users\\prompt/.cache\\torch\\hub\\master.zip\n",
      "C:\\Users\\prompt\\anaconda3\\envs\\PD\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\prompt\\anaconda3\\envs\\PD\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "    'AdeelH/pytorch-fpn',\n",
    "    'make_fpn_resnet',\n",
    "    force_reload=True,\n",
    "    name='resnet50',\n",
    "    fpn_type='panet',\n",
    "    num_classes=2,\n",
    "    fpn_channels=256,\n",
    "    in_channels=3,\n",
    "    out_size=(224, 224)\n",
    ")\n",
    "\n",
    "# for weight in model.parameters():\n",
    "#     weight.requires_grad = False\n",
    "\n",
    "\n",
    "class CustomYolo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomYolo,self).__init__()\n",
    "        self.model = model\n",
    "        self.flat_layer = nn.Flatten(0,-1)\n",
    "\n",
    "\n",
    "        # output tensor :\n",
    "        # SS: Grid cells: 11*11\n",
    "        # B: Bounding box per grid cell: 2\n",
    "        # C: classes: 3\n",
    "        # Coords: x, y, w, h per box: 4\n",
    "        # tensor length: SS * (C +B(5) ) : 363--242--968 => 1573\n",
    "        out_dim = 11*11*(3+2*5)\n",
    "        self.dense_0 = nn.Linear(100352, out_dim)\n",
    "        # self._fc = nn.Linear(1024,200, bias=False)\n",
    "    def forward(self, x):\n",
    "        batch_size ,_,_,_ =x.shape\n",
    "        x = self.model(x)\n",
    "        x = self.flat_layer(x)\n",
    "        x = self.dense_0(x)\n",
    "        # x = self.model.avgpool(x)\n",
    "        # x = x.view(-1, 1024)\n",
    "        # x = self.layernorm(x)\n",
    "        # x = self._fc(x)\n",
    "        # x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2batch [00:00,  5.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4batch [00:00,  5.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6batch [00:01,  6.16batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8batch [00:01,  6.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10batch [00:01,  6.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12batch [00:01,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14batch [00:02,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16batch [00:02,  6.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18batch [00:02,  6.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20batch [00:03,  6.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22batch [00:03,  6.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24batch [00:03,  6.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26batch [00:04,  6.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28batch [00:04,  6.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30batch [00:04,  6.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32batch [00:04,  6.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34batch [00:05,  6.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36batch [00:05,  6.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38batch [00:05,  6.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40batch [00:06,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42batch [00:06,  6.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44batch [00:06,  6.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46batch [00:07,  6.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48batch [00:07,  6.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50batch [00:07,  6.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52batch [00:08,  6.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54batch [00:08,  6.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56batch [00:08,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor(0.0068, grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58batch [00:08,  6.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60batch [00:09,  6.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62batch [00:09,  6.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64batch [00:09,  6.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66batch [00:10,  6.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68batch [00:10,  6.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70batch [00:10,  6.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.0063, grad_fn=<DivBackward0>)]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72batch [00:11,  6.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74batch [00:11,  6.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76batch [00:11,  6.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78batch [00:12,  6.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80batch [00:12,  6.44batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82batch [00:12,  6.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84batch [00:12,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86batch [00:13,  6.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88batch [00:13,  6.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90batch [00:13,  6.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92batch [00:14,  6.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94batch [00:14,  6.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96batch [00:14,  6.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98batch [00:15,  6.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100batch [00:15,  6.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102batch [00:15,  6.71batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104batch [00:15,  6.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106batch [00:16,  6.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108batch [00:16,  6.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110batch [00:16,  6.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.0010, grad_fn=<DivBackward0>)]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112batch [00:17,  6.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114batch [00:17,  6.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116batch [00:17,  6.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118batch [00:18,  6.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120batch [00:18,  6.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122batch [00:18,  6.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124batch [00:19,  6.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126batch [00:19,  6.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128batch [00:19,  6.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130batch [00:19,  6.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132batch [00:20,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134batch [00:20,  6.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136batch [00:20,  6.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136batch [00:20,  6.50batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    106\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 108\u001b[0m train_prob \u001b[38;5;241m=\u001b[39m \u001b[43myolo_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m train_prob \u001b[38;5;241m=\u001b[39m train_prob\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    111\u001b[0m train_prob_segments \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    112\u001b[0m         train_prob[:segment_lengths[\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    113\u001b[0m         train_prob[segment_lengths[\u001b[38;5;241m0\u001b[39m]:segment_lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m segment_lengths[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m    114\u001b[0m         train_prob[segment_lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m segment_lengths[\u001b[38;5;241m1\u001b[39m]:]\n\u001b[0;32m    115\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 35\u001b[0m, in \u001b[0;36mCustomYolo.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     batch_size ,_,_,_ \u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_layer(x)\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_0(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\AdeelH_pytorch-fpn_master\\fpn\\backbone.py:59\u001b[0m, in \u001b[0;36mResNetFeatureMapsExtractor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfusion\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     x, in_feats \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm((x, \u001b[38;5;241m*\u001b[39min_feats))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\AdeelH_pytorch-fpn_master\\fpn\\containers.py:50\u001b[0m, in \u001b[0;36mSequentialMultiOutput.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m last_out \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 50\u001b[0m     last_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     outs[i] \u001b[38;5;241m=\u001b[39m last_out\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(outs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torchvision\\models\\resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PD\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_csv = pd.DataFrame(columns=[\"Loss\", \"IOU\"])\n",
    "test_loss_csv = pd.DataFrame(columns=[\"Loss\", \"IOU\"])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "WIDTH_NORM = 224\n",
    "HEIGHT_NORM = 224\n",
    "GRID_NUM = 11\n",
    "X_SPAN = WIDTH_NORM/GRID_NUM\n",
    "Y_SPAN = HEIGHT_NORM/GRID_NUM\n",
    "X_NORM = WIDTH_NORM/GRID_NUM\n",
    "Y_NORM = HEIGHT_NORM/GRID_NUM\n",
    "\n",
    "weights_path = 'imagenet'\n",
    "save_prefix = 'run1_'\n",
    "learning_rate = 1e-2\n",
    "if len(sys.argv) > 3:\n",
    "    weights_path = sys.argv[1]\n",
    "    save_prefix = sys.argv[2]\n",
    "    learning_rate = float(sys.argv[3])\n",
    "elif len(sys.argv) > 2:\n",
    "    weights_path = sys.argv[1]\n",
    "    save_prefix = sys.argv[2]\n",
    "elif len(sys.argv) > 1:\n",
    "    weights_path = sys.argv[1]\n",
    "\n",
    "\n",
    "yolo_resnet = CustomYolo()\n",
    "\n",
    "tfm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "yolo_resnet = yolo_resnet.to(device)\n",
    "\n",
    "#TODO Change the file path here if needed\n",
    "with open(f\"D:\\PD Data\\Resnet+yolo Code\\label_frames_plant_disease_final_v_train (1).p\", 'rb') as f:\n",
    "    label_frames = pickle.load(f)\n",
    "\n",
    "label_keys = list(label_frames.keys())\n",
    "lbl_train, lbl_validn = train_test_split(label_keys, test_size=0.2)\n",
    "\n",
    "LEN_TRAIN = len(lbl_train)\n",
    "LEN_TEST = len(lbl_validn)\n",
    "\n",
    "\n",
    "### Intialize generator\n",
    "optimiser = Adam(yolo_resnet.parameters(), lr=1e-2, weight_decay=0.01)\n",
    "\n",
    "\n",
    "# training_image = torch.from_numpy(train_generator[0])\n",
    "# training_image = training_image.permute(0, 3, 1, 2)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_iou_list = []\n",
    "test_iou_list = []\n",
    "\n",
    "\n",
    "segment_lengths = [363, 242, 968]\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    # start = time()\n",
    "\n",
    "    tr_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    # Train\n",
    "    yolo_resnet.train()\n",
    "    # count = 0\n",
    "\n",
    "    # model_weights_path = f\"yolo_resnet_weights_epoch{epoch + 1}.pth\"\n",
    "    # torch.save(yolo_resnet.state_dict(), model_weights_path)\n",
    "    # print(f\"Model weights saved for Epoch {epoch + 1}\")\n",
    "\n",
    "    train_generator = generator(lbl_train, label_frames)\n",
    "\n",
    "    with tqdm(train_generator, unit=\"batch\") as tepoch:\n",
    "        train_loss_for_mean = []\n",
    "        for x_train, y_train in tepoch:\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            y_train_segments = [\n",
    "                    y_train[0][:segment_lengths[0]],\n",
    "                    y_train[0][segment_lengths[0]:segment_lengths[0] + segment_lengths[1]],\n",
    "                    y_train[0][segment_lengths[0] + segment_lengths[1]:]\n",
    "                ]\n",
    "\n",
    "\n",
    "            class_probabilities_actual = y_train_segments[0]  # Segment 1\n",
    "            confidence_actual = y_train_segments[1]  # Segment 2\n",
    "            bounding_boxes_actual = y_train_segments[2]  # Segment 3\n",
    "\n",
    "            bounding_boxes_grouped_actual = bounding_boxes_actual.reshape(-1,2,4)\n",
    "            class_probabilities_grouped_actual = class_probabilities_actual.reshape(-1, 3)\n",
    "            confidence_grouped_actual = confidence_actual.reshape(-1,2)\n",
    "\n",
    "\n",
    "            x_train = x_train.permute(0, 3, 1, 2)\n",
    "            x_train = x_train.to(device)\n",
    "\n",
    "            train_prob = yolo_resnet(x_train)\n",
    "            train_prob = train_prob.cpu()\n",
    "\n",
    "            train_prob_segments = [\n",
    "                    train_prob[:segment_lengths[0]],\n",
    "                    train_prob[segment_lengths[0]:segment_lengths[0] + segment_lengths[1]],\n",
    "                    train_prob[segment_lengths[0] + segment_lengths[1]:]\n",
    "                ]\n",
    "\n",
    "\n",
    "            class_probabilities_predicted = train_prob_segments[0]  # Segment 1\n",
    "            confidence_predicted = train_prob_segments[1]  # Segment 2\n",
    "            bounding_boxes_predicted = train_prob_segments[2]  # Segment 3\n",
    "\n",
    "            bounding_boxes_grouped_predicted = bounding_boxes_predicted.reshape(-1,2,4)\n",
    "            class_probabilities_grouped_predicted = class_probabilities_predicted.reshape(-1, 3)\n",
    "            confidence_grouped_predicted = confidence_predicted.reshape(-1,2)\n",
    "\n",
    "\n",
    "            train_loss = custom_loss(train_prob, y_train)\n",
    "            train_loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss_for_mean.append(train_loss)\n",
    "\n",
    "            \n",
    "                        # Inside the loop where you calculate IoU\n",
    "            if bounding_boxes_grouped_predicted.numel() == 0:\n",
    "                print(\"No predicted bounding boxes.\")\n",
    "                average_train_iou = 0\n",
    "            else:\n",
    "                iou_list = find_average_iou(bounding_boxes_grouped_actual, bounding_boxes_grouped_predicted)\n",
    "                average_train_iou = 0 if len(iou_list) == 0 else sum(iou_list) / len(iou_list)\n",
    "                print(iou_list)\n",
    "            train_iou_list.append(average_train_iou)\n",
    "\n",
    "            # iou_list = find_average_iou(bounding_boxes_grouped_actual, bounding_boxes_grouped_predicted)\n",
    "\n",
    "            # if len(iou_list) == 0:\n",
    "            #     print(\"iou_list = 0\")\n",
    "            #     average_train_iou = 0\n",
    "            # else:\n",
    "            #     average_train_iou = sum(iou_list)/len(iou_list)\n",
    "            #     print(iou_list)\n",
    "            # train_iou_list.append(average_train_iou)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # training ends\n",
    "            # train_pred = torch.max(train_prob, 1).indices\n",
    "            # tr_acc += int(torch.sum(train_pred == y_train))\n",
    "\n",
    "    average_train_loss = sum(train_loss_for_mean)/len(train_loss_for_mean)\n",
    "    train_losses.append(average_train_loss)\n",
    "\n",
    "    # ep_tr_acc = tr_acc / LEN_TRAIN\n",
    "\n",
    "\n",
    "        # ep_tr_acc = tr_acc / LEN_TRAIN\n",
    "\n",
    "    # Evaluate\n",
    "    validation_generator = generator(lbl_validn, label_frames)\n",
    "\n",
    "    yolo_resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss_for_mean = []\n",
    "        for xtest, ytest in validation_generator:\n",
    "            xtest = xtest.to(device)\n",
    "            xtest = xtest.permute(0, 3, 1, 2)\n",
    "\n",
    "            y_test_segments = [\n",
    "                    ytest[0][:segment_lengths[0]],\n",
    "                    ytest[0][segment_lengths[0]:segment_lengths[0] + segment_lengths[1]],\n",
    "                    ytest[0][segment_lengths[0] + segment_lengths[1]:]\n",
    "                ]\n",
    "\n",
    "\n",
    "            class_probabilities_actual = y_test_segments[0]  # Segment 1\n",
    "            confidence_actual = y_test_segments[1]  # Segment 2\n",
    "            bounding_boxes_actual = y_test_segments[2]  # Segment 3\n",
    "\n",
    "            bounding_boxes_grouped_actual = bounding_boxes_actual.reshape(-1,2,4)\n",
    "            class_probabilities_grouped_actual = class_probabilities_actual.reshape(-1, 3)\n",
    "            confidence_grouped_actual = confidence_actual.reshape(-1,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            test_prob = yolo_resnet(xtest)\n",
    "            test_prob = test_prob.cpu()\n",
    "\n",
    "\n",
    "\n",
    "            test_prob_segments = [\n",
    "                    test_prob[:segment_lengths[0]],\n",
    "                    test_prob[segment_lengths[0]:segment_lengths[0] + segment_lengths[1]],\n",
    "                    test_prob[segment_lengths[0] + segment_lengths[1]:]\n",
    "                ]\n",
    "\n",
    "\n",
    "            class_probabilities_predicted = test_prob_segments[0]  # Segment 1\n",
    "            confidence_predicted = test_prob_segments[1]  # Segment 2\n",
    "            bounding_boxes_predicted = test_prob_segments[2]  # Segment 3\n",
    "\n",
    "            bounding_boxes_grouped_predicted = bounding_boxes_predicted.reshape(-1,2,4)\n",
    "            class_probabilities_grouped_predicted = class_probabilities_predicted.reshape(-1, 3)\n",
    "            confidence_grouped_predicted = confidence_predicted.reshape(-1,2)\n",
    "\n",
    "\n",
    "            test_loss = custom_loss(test_prob, ytest)\n",
    "\n",
    "            test_loss_for_mean.append(test_loss)\n",
    "\n",
    "\n",
    "            iou_list = find_average_iou(bounding_boxes_grouped_actual, bounding_boxes_grouped_predicted)\n",
    "\n",
    "\n",
    "            if len(iou_list) == 0:\n",
    "                print(\"iou_list = 0\")\n",
    "                average_test_iou = 0\n",
    "            else:\n",
    "              average_test_iou = sum(iou_list)/len(iou_list)\n",
    "              print(iou_list)\n",
    "            test_iou_list.append(average_test_iou)\n",
    "\n",
    "\n",
    "            # test_pred = torch.max(test_prob,1).indices\n",
    "            # test_acc += int(torch.sum(test_pred == ytest))\n",
    "\n",
    "\n",
    "    average_test_loss = sum(test_loss_for_mean)/len(test_loss_for_mean)\n",
    "    test_losses.append(average_test_loss)\n",
    "\n",
    "    # Calculate and pprint validation accuracy\n",
    "    print(f\"Epoch:- {epoch+1}, Train Loss:- {average_train_loss}, Test Loss:- {average_test_loss}, Train IOU:- {average_train_iou}, Test IOU:- {average_test_iou}\")\n",
    "\n",
    "\n",
    "\n",
    "    epc_train_df = pd.DataFrame({\"Loss\":[average_train_loss.detach().numpy()],\"IOU\":[average_train_iou.numpy()]})\n",
    "    train_loss_csv = pd.concat([train_loss_csv, epc_train_df], ignore_index = True)\n",
    "    train_loss_csv.to_csv(\"yolo_resnet_trn_loss_1.csv\")\n",
    "\n",
    "    epc_test_df = pd.DataFrame({\"Loss\":[average_test_loss.detach().numpy()],\"IOU\":[average_test_iou.numpy()]})\n",
    "\n",
    "    test_loss_csv = pd.concat([test_loss_csv, epc_test_df], ignore_index = True)\n",
    "    test_loss_csv.to_csv(\"yolo_resnet_test_loss_1.csv\")\n",
    "\n",
    "    # with open('yolo_model.pkl', 'wb') as file:\n",
    "    #   # A new file will be created\n",
    "    #   pickle.dump(yolo_resnet, file)\n",
    "\n",
    "\n",
    "\n",
    "# print(type(), train_generator[1].shape)\n",
    "# print(yolo_resnet(training_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDklnOQ+BoknKjDzQEbw1+",
   "mount_file_id": "1DXVAen6W63FADyehYbolKSE2K5pphHyO",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
